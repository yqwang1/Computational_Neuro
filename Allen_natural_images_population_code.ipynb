{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yqwang1/Computational_Neuro/blob/main/Allen_natural_images_population_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the data, copy the file at https://drive.google.com/file/d/1jp1NRQuidfjRkwJUVd59pLmmzn8oPeC5/view?usp=sharing\n",
        " into your colab drive folder then run the next cell"
      ],
      "metadata": {
        "id": "FhG21NyaiSuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip siegle_791319847.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dygC_0lsi3yy",
        "outputId": "e9ddf440-afb7-48c9-e269-8c88ab3c88a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  siegle_791319847.zip\n",
            "  inflating: All_images.npy          \n",
            "  inflating: clusters.brainLocationAcronyms_ccf_2017.npy  \n",
            "  inflating: frame_plus_one.spike_histograms.npy  \n",
            "  inflating: stims_natural_scenes.frame.npy  \n",
            "  inflating: stims_natural_scenes.intervals.npy  \n",
            "  inflating: stims_natural_scenes.repeat.npy  \n",
            "  inflating: stims_natural_scenes.spike_counts.npy  \n",
            "  inflating: stims_natural_scenes.stimulus_block.npy  \n",
            "  inflating: stims_natural_scenes.stimulus_condition_id.npy  \n",
            "  inflating: units.csv               \n",
            "  inflating: neurocode.py            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu3SE_Eihr4F",
        "outputId": "64b5af87-661a-41ab-8be8-2973eb51043c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rastermap\n",
            "  Downloading rastermap-1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from rastermap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from rastermap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from rastermap) (1.6.1)\n",
            "Requirement already satisfied: numba>=0.57.0 in /usr/local/lib/python3.11/dist-packages (from rastermap) (0.61.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from rastermap) (8.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from rastermap) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57.0->rastermap) (0.44.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->rastermap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->rastermap) (3.5.0)\n",
            "Downloading rastermap-1.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.5/90.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rastermap\n",
            "Successfully installed rastermap-1.0\n"
          ]
        }
      ],
      "source": [
        "pip install rastermap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import neurocode as nc\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from rastermap import Rastermap\n"
      ],
      "metadata": {
        "id": "fsbSqMY6h3Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "FileDir = r'.'\n",
        "Experiment = ''\n",
        "\n",
        "stims=nc.load_object(os.path.join(FileDir, Experiment, 'stims_natural_scenes'))\n",
        "frame_plus_one = nc.load_object(os.path.join(FileDir, Experiment, 'frame_plus_one'))\n",
        "clusters = nc.load_object(os.path.join(FileDir, Experiment, 'clusters'))\n",
        "\n"
      ],
      "metadata": {
        "id": "jQbOhSk5NUUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frame_plus_one.spike_counts: size nFrames x nClusters x nRepeats\n",
        "# Frame here means what stimulus was shown\n",
        "# we use frame_plus_one because Allen use -1 to mean the blank frame\n",
        "# sorts spike counts into 3d array\n",
        "\n",
        "nStims = len(stims)\n",
        "nClusters = len(clusters)\n",
        "nFrames = len(frame_plus_one)\n",
        "nRepeats = 50\n",
        "\n",
        "frame_plus_one.spike_counts = np.histogramdd([stims.frame.repeat(nClusters)+1, np.tile(np.arange(nClusters),nStims),\n",
        "                                              stims.repeat.repeat(nClusters)],\n",
        "                    bins=[np.arange(nFrames+1), np.arange(nClusters+1), np.arange(nRepeats+1)],\n",
        "                    weights=stims.spike_counts.ravel()\n",
        "                  )[0]"
      ],
      "metadata": {
        "id": "sUspQDech9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show stimuli\n",
        "images = np.load('All_images.npy')\n",
        "im0 = images[0,:,:]\n",
        "fig, ax = plt.subplots(7,17,figsize=(34*1.174,14*.918), sharex=True, sharey=True, gridspec_kw={'wspace':.05, 'hspace':.05})\n",
        "for i in np.arange(-1,118):\n",
        "    plt.sca(ax.flat[i+1])\n",
        "    if i==-1:\n",
        "        plt.imshow(128*np.ones_like(im0), cmap='gray',vmin=0,vmax=255)\n",
        "    else:\n",
        "        im = images[i,:,:]\n",
        "        plt.imshow(im, cmap='gray',vmin=0,vmax=255)\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "MoC7Aah_NfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the mean responses of all cells to all stimuli with Rastermap\n",
        "frame_plus_one.mean_spkcnt = frame_plus_one.spike_counts.mean(2)\n",
        "model = Rastermap().fit(frame_plus_one.mean_spkcnt.T) # sort so correlated neurons are close\n",
        "model2 = Rastermap().fit(model.X_embedding.T) # sort so correlated stimuli are close\n",
        "\n",
        "plt.imshow(model2.X_embedding, vmin=0, vmax=3, cmap=\"gray_r\")\n",
        "plt.colorbar()\n",
        "plt.xlabel('Cell')\n",
        "plt.ylabel('Stimulus')"
      ],
      "metadata": {
        "id": "Qu3sPpxQNohP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data, split into training and testing sets\n",
        "\n",
        "X = stims.spike_counts\n",
        "y = stims.frame + 1  # because stims.frame starts from -1\n",
        "\n",
        "train = stims.repeat<40\n",
        "test = stims.repeat>=40\n",
        "\n",
        "X_train = X[train,:]\n",
        "X_test = X[test,:]\n",
        "\n",
        "y_train = y[train]\n",
        "y_test = y[test]"
      ],
      "metadata": {
        "id": "YJ8egrnbPalk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Nearest-neighbor decoding accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "PqvzmC6XTf-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Permutation test to see if prediction is significantly better than random\n",
        "n_permutations = 999\n",
        "permuted_accuracies = np.zeros(n_permutations)\n",
        "\n",
        "for i in range(n_permutations):\n",
        "    # Shuffle the test set labels (y_test)\n",
        "    y_test_permuted = np.random.permutation(y_test)\n",
        "\n",
        "    # Calculate accuracy on permuted test data (using original trained model)\n",
        "    permuted_accuracies[i] = accuracy_score(y_test_permuted, y_pred)\n",
        "\n",
        "# Calculate p-value (same as before)\n",
        "p_value = (np.sum(permuted_accuracies >= accuracy) + 1) / (n_permutations + 1)\n",
        "\n",
        "print(f\"Permutation test p-value: {p_value}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4YlvGuDra9B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION: why is permuting the test set labels a better idea than permuting the training set labels?"
      ],
      "metadata": {
        "id": "mXUCxeJbhDUI"
      }
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic regression is classical statistical method that can be thought of as\n",
        "# a 1-layer neural network\n",
        "\n",
        "# Create and train the logistic regression classifier\n",
        "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Decode stimuli\n",
        "y_pred = lr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic regression decoding accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U10N1Il6YeGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE: Run a permutation test on the results of the LR classifier"
      ],
      "metadata": {
        "id": "nHVlh2Ilc4H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Permutation test (permuting test set)\n",
        "n_permutations = 999\n",
        "permuted_accuracies = np.zeros(n_permutations)\n",
        "\n",
        "for i in range(n_permutations):\n",
        "    # Shuffle the test set labels (y_test)\n",
        "    y_test_permuted = np.random.permutation(y_test)\n",
        "\n",
        "    # Calculate accuracy on permuted test data (using original trained model)\n",
        "    permuted_accuracies[i] = accuracy_score(y_test_permuted, y_pred)\n",
        "\n",
        "# Calculate p-value (same as before)\n",
        "p_value = (np.sum(permuted_accuracies >= accuracy) + 1) / (n_permutations + 1)\n",
        "\n",
        "print(f\"Permutation test p-value: {p_value}\")"
      ],
      "metadata": {
        "id": "XNRhMMw_dfrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE: run the logistic regression classifier predicting from a random selection of 10 cells.  Is it still significant?\n",
        "\n",
        "How few cells do you need to have before you lose signifiance?\n",
        "\n",
        "HINT: run this multiple times.  why can you get a different p-value each time?\n"
      ],
      "metadata": {
        "id": "kr5_ve4YdVzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_cells = 5 #\n",
        "use_cells = np.random.choice(nClusters, size=n_cells, replace=False)\n",
        "\n",
        "# Logistic regression is classical statistical method that can be thought of as\n",
        "# a 1-layer neural network\n",
        "\n",
        "# Create and train the logistic regression classifier\n",
        "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "lr_classifier.fit(X_train[:,use_cells], y_train)\n",
        "\n",
        "# Decode stimuli\n",
        "y_pred = lr_classifier.predict(X_test[:,use_cells])\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic regression decoding accuracy: {accuracy}\")\n",
        "\n",
        "for i in range(n_permutations):\n",
        "    # Shuffle the test set labels (y_test)\n",
        "    y_test_permuted = np.random.permutation(y_test)\n",
        "\n",
        "    # Calculate accuracy on permuted test data (using original trained model)\n",
        "    permuted_accuracies[i] = accuracy_score(y_test_permuted, y_pred)\n",
        "\n",
        "# Calculate p-value (same as before)\n",
        "p_value = (np.sum(permuted_accuracies >= accuracy) + 1) / (n_permutations + 1)\n",
        "\n",
        "print(f\"Permutation test p-value: {p_value}\")"
      ],
      "metadata": {
        "id": "DBg3asQaddq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Now we will try a neural network classifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Create and train the backpropagation classifier (MLPClassifier)\n",
        "bp_classifier = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
        "                             max_iter=1000, random_state=0)\n",
        "bp_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Decode stimuli\n",
        "y_pred = bp_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Neural network decoding accuracy: {accuracy}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S_r7Ni4SUwY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXERCISE: try running the neural network classifier with fewer cells. How well does it work?  Can you get it to be better than logistic regression?"
      ],
      "metadata": {
        "id": "_TZY7yLMc9ym"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRQZl1OAZ8Cj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}